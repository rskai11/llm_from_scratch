{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8466c4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"Data/the-verdict.txt\", \"r\") as file:\n",
    "    raw_text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75551dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20479\n"
     ]
    }
   ],
   "source": [
    "print(len(raw_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b989c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c58964b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"Hello, world! This is a test-text for tokenization.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b9b6e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "result=re.split(r'(\\s)', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8bd404b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', ' ', 'world!', ' ', 'This', ' ', 'is', ' ', 'a', ' ', 'test-text', ' ', 'for', ' ', 'tokenization.']\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2877ea07",
   "metadata": {},
   "outputs": [],
   "source": [
    "result=re.split(r'([,.]|\\s)',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5cdb71e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world!', ' ', 'This', ' ', 'is', ' ', 'a', ' ', 'test-text', ' ', 'for', ' ', 'tokenization', '.', '']\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e3ef8232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "pattern = r'(--|[,\\.\\-?!_;:\\'\"(){}\\[\\]<>/\\\\@#$%^&*+=]|\\s)'\n",
    "result = re.split(pattern, raw_text)\n",
    "# keep only non-whitespace tokens and drop tokens that are purely punctuation\n",
    "preprocessed = [item.strip() for item in result if item.strip()]\n",
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "166aa73e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4766"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "84a53eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1140\n"
     ]
    }
   ],
   "source": [
    "all_words=sorted(set(preprocessed))\n",
    "vocab_size=len(all_words)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2aa18174",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "88c5876f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'!': 0, '\"': 1, \"'\": 2, '(': 3, ')': 4, ',': 5, '-': 6, '--': 7, '.': 8, ':': 9, ';': 10, '?': 11, 'A': 12, 'Ah': 13, 'Among': 14, 'And': 15, 'Are': 16, 'Arrt': 17, 'As': 18, 'At': 19, 'Be': 20, 'Begin': 21, 'Burlington': 22, 'But': 23, 'By': 24, 'Carlo': 25, 'Chicago': 26, 'Claude': 27, 'Come': 28, 'Croft': 29, 'Destroyed': 30, 'Devonshire': 31, 'Don': 32, 'Dubarry': 33, 'Emperors': 34, 'Florence': 35, 'For': 36, 'Gallery': 37, 'Gideon': 38, 'Gisburn': 39, 'Gisburns': 40, 'Grafton': 41, 'Greek': 42, 'Grindle': 43, 'Grindles': 44, 'HAD': 45, 'Had': 46, 'Hang': 47, 'Has': 48, 'He': 49, 'Her': 50, 'Hermia': 51, 'His': 52, 'How': 53, 'I': 54, 'If': 55, 'In': 56, 'It': 57, 'Jack': 58, 'Jove': 59, 'Just': 60, 'Lord': 61, 'Made': 62, 'Miss': 63, 'Money': 64, 'Monte': 65, 'Moon': 66, 'Mr': 67, 'Mrs': 68, 'My': 69, 'Never': 70, 'No': 71, 'Now': 72, 'Nutley': 73, 'Of': 74, 'Oh': 75, 'On': 76, 'Once': 77, 'Only': 78, 'Or': 79, 'Perhaps': 80, 'Poor': 81, 'Professional': 82, 'Renaissance': 83, 'Rickham': 84, 'Riviera': 85, 'Rome': 86, 'Russian': 87, 'Sevres': 88, 'She': 89, 'Stroud': 90, 'Strouds': 91, 'Suddenly': 92, 'That': 93, 'The': 94, 'Then': 95, 'There': 96, 'They': 97, 'This': 98, 'Those': 99, 'Though': 100, 'Thwing': 101, 'Thwings': 102, 'To': 103, 'Usually': 104, 'Venetian': 105, 'Victor': 106, 'Was': 107, 'We': 108, 'Well': 109, 'What': 110, 'When': 111, 'Why': 112, 'Yes': 113, 'You': 114, '_': 115, 'a': 116, 'abdication': 117, 'able': 118, 'about': 119, 'above': 120, 'abruptly': 121, 'absolute': 122, 'absorbed': 123, 'absurdity': 124, 'academic': 125, 'accuse': 126, 'accustomed': 127, 'across': 128, 'activity': 129, 'add': 130, 'added': 131, 'admirers': 132, 'adopted': 133, 'adulation': 134, 'advance': 135, 'aesthetic': 136, 'affect': 137, 'afraid': 138, 'after': 139, 'afterward': 140, 'again': 141, 'ago': 142, 'ah': 143, 'air': 144, 'alive': 145, 'all': 146, 'almost': 147, 'alone': 148, 'along': 149, 'always': 150, 'am': 151, 'amazement': 152, 'amid': 153, 'among': 154, 'amplest': 155, 'amusing': 156, 'an': 157, 'and': 158, 'another': 159, 'answer': 160, 'answered': 161, 'any': 162, 'anything': 163, 'anywhere': 164, 'apparent': 165, 'apparently': 166, 'appearance': 167, 'appeared': 168, 'appointed': 169, 'are': 170, 'arm': 171, 'arms': 172, 'art': 173, 'articles': 174, 'artist': 175, 'as': 176, 'aside': 177, 'asked': 178, 'at': 179, 'atmosphere': 180, 'atom': 181, 'attack': 182, 'attention': 183, 'attitude': 184, 'audacities': 185, 'away': 186, 'awful': 187, 'axioms': 188, 'azaleas': 189, 'back': 190, 'background': 191, 'balance': 192, 'balancing': 193, 'balustraded': 194, 'basking': 195, 'bath': 196, 'be': 197, 'beaming': 198, 'bean': 199, 'bear': 200, 'beard': 201, 'beauty': 202, 'became': 203, 'because': 204, 'becoming': 205, 'bed': 206, 'been': 207, 'before': 208, 'began': 209, 'begun': 210, 'behind': 211, 'being': 212, 'believed': 213, 'beneath': 214, 'bespoke': 215, 'better': 216, 'between': 217, 'big': 218, 'bits': 219, 'bitterness': 220, 'blocked': 221, 'born': 222, 'borne': 223, 'boudoir': 224, 'brac': 225, 'bravura': 226, 'break': 227, 'breaking': 228, 'breathing': 229, 'breeding': 230, 'bric': 231, 'briefly': 232, 'brings': 233, 'bronzes': 234, 'brought': 235, 'brown': 236, 'brush': 237, 'bull': 238, 'business': 239, 'but': 240, 'buying': 241, 'by': 242, 'called': 243, 'came': 244, 'can': 245, 'canvas': 246, 'canvases': 247, 'cards': 248, 'care': 249, 'career': 250, 'caught': 251, 'central': 252, 'century': 253, 'chair': 254, 'chairs': 255, 'chap': 256, 'characteristic': 257, 'charming': 258, 'cheap': 259, 'check': 260, 'cheeks': 261, 'chest': 262, 'chimney': 263, 'chucked': 264, 'cigar': 265, 'cigarette': 266, 'cigars': 267, 'circulation': 268, 'circumstance': 269, 'circus': 270, 'claimed': 271, 'clasping': 272, 'clear': 273, 'cleverer': 274, 'close': 275, 'closets': 276, 'clown': 277, 'clue': 278, 'coat': 279, 'collapsed': 280, 'colour': 281, 'come': 282, 'comfortable': 283, 'coming': 284, 'companion': 285, 'compared': 286, 'complex': 287, 'confident': 288, 'congesting': 289, 'conjugal': 290, 'constraint': 291, 'consummate': 292, 'contended': 293, 'continued': 294, 'corner': 295, 'corrected': 296, 'cotta': 297, 'could': 298, 'couldn': 299, 'count': 300, 'countenance': 301, 'couple': 302, 'course': 303, 'covered': 304, 'craft': 305, 'cried': 306, 'crossed': 307, 'crowned': 308, 'crumbled': 309, 'cry': 310, 'cured': 311, 'curiosity': 312, 'curious': 313, 'current': 314, 'curtains': 315, 'd': 316, 'dabble': 317, 'damask': 318, 'dancers': 319, 'dark': 320, 'dashed': 321, 'day': 322, 'days': 323, 'dead': 324, 'deadening': 325, 'dear': 326, 'deep': 327, 'deerhound': 328, 'degree': 329, 'delicate': 330, 'demand': 331, 'denied': 332, 'deploring': 333, 'deprecating': 334, 'deprecatingly': 335, 'desire': 336, 'destroyed': 337, 'destruction': 338, 'desultory': 339, 'detail': 340, 'diagnosis': 341, 'did': 342, 'didn': 343, 'died': 344, 'dim': 345, 'dimmest': 346, 'dingy': 347, 'dining': 348, 'disarming': 349, 'discovery': 350, 'discrimination': 351, 'discussion': 352, 'disdain': 353, 'disdained': 354, 'disease': 355, 'disguised': 356, 'display': 357, 'dissatisfied': 358, 'distinguished': 359, 'distract': 360, 'divert': 361, 'do': 362, 'doesn': 363, 'doing': 364, 'domestic': 365, 'don': 366, 'done': 367, 'donkey': 368, 'down': 369, 'dozen': 370, 'dragged': 371, 'drawing': 372, 'drawn': 373, 'dress': 374, 'drew': 375, 'dropped': 376, 'each': 377, 'earth': 378, 'ease': 379, 'easel': 380, 'easy': 381, 'echoed': 382, 'economy': 383, 'effect': 384, 'effects': 385, 'efforts': 386, 'egregious': 387, 'eighteenth': 388, 'elbow': 389, 'elegant': 390, 'else': 391, 'embarrassed': 392, 'enabled': 393, 'end': 394, 'endless': 395, 'enjoy': 396, 'enlightenment': 397, 'enough': 398, 'ensuing': 399, 'equally': 400, 'equanimity': 401, 'escape': 402, 'established': 403, 'etching': 404, 'even': 405, 'event': 406, 'ever': 407, 'everlasting': 408, 'every': 409, 'exasperated': 410, 'except': 411, 'excuse': 412, 'excusing': 413, 'existed': 414, 'expected': 415, 'exquisite': 416, 'exquisitely': 417, 'extenuation': 418, 'exterminating': 419, 'extracting': 420, 'eye': 421, 'eyebrows': 422, 'eyes': 423, 'face': 424, 'faces': 425, 'fact': 426, 'faded': 427, 'failed': 428, 'failure': 429, 'fair': 430, 'faith': 431, 'false': 432, 'familiar': 433, 'famille': 434, 'fancy': 435, 'fashionable': 436, 'fate': 437, 'feather': 438, 'feet': 439, 'fell': 440, 'fellow': 441, 'felt': 442, 'few': 443, 'fewer': 444, 'finality': 445, 'find': 446, 'fingers': 447, 'first': 448, 'fit': 449, 'fitting': 450, 'five': 451, 'flash': 452, 'flashed': 453, 'florid': 454, 'flowers': 455, 'fluently': 456, 'flung': 457, 'follow': 458, 'followed': 459, 'fond': 460, 'footstep': 461, 'for': 462, 'forced': 463, 'forcing': 464, 'forehead': 465, 'foreign': 466, 'foreseen': 467, 'forgive': 468, 'forgotten': 469, 'form': 470, 'formed': 471, 'forming': 472, 'forward': 473, 'fostered': 474, 'found': 475, 'foundations': 476, 'four': 477, 'fragment': 478, 'fragments': 479, 'frame': 480, 'frames': 481, 'frequently': 482, 'friend': 483, 'from': 484, 'full': 485, 'fullest': 486, 'furiously': 487, 'furrowed': 488, 'garlanded': 489, 'garlands': 490, 'gave': 491, 'genial': 492, 'genius': 493, 'gesture': 494, 'get': 495, 'getting': 496, 'give': 497, 'given': 498, 'glad': 499, 'glanced': 500, 'glimpse': 501, 'gloried': 502, 'glory': 503, 'go': 504, 'going': 505, 'gone': 506, 'good': 507, 'got': 508, 'grace': 509, 'gradually': 510, 'gray': 511, 'grayish': 512, 'great': 513, 'greatest': 514, 'greatness': 515, 'grew': 516, 'groping': 517, 'growing': 518, 'had': 519, 'hadn': 520, 'hair': 521, 'half': 522, 'hall': 523, 'hand': 524, 'hands': 525, 'handsome': 526, 'hanging': 527, 'happen': 528, 'happened': 529, 'hard': 530, 'hardly': 531, 'has': 532, 'have': 533, 'haven': 534, 'having': 535, 'he': 536, 'head': 537, 'hear': 538, 'heard': 539, 'heart': 540, 'height': 541, 'her': 542, 'here': 543, 'hermit': 544, 'herself': 545, 'hesitations': 546, 'hide': 547, 'high': 548, 'him': 549, 'himself': 550, 'hint': 551, 'his': 552, 'history': 553, 'holding': 554, 'home': 555, 'honour': 556, 'hooded': 557, 'hostess': 558, 'hot': 559, 'hour': 560, 'hours': 561, 'house': 562, 'how': 563, 'humoured': 564, 'hung': 565, 'husband': 566, 'idea': 567, 'idle': 568, 'idling': 569, 'if': 570, 'immediately': 571, 'in': 572, 'incense': 573, 'indifferent': 574, 'inevitable': 575, 'inevitably': 576, 'inflexible': 577, 'insensible': 578, 'insignificant': 579, 'instinctively': 580, 'instructive': 581, 'interesting': 582, 'into': 583, 'ironic': 584, 'irony': 585, 'irrelevance': 586, 'irrevocable': 587, 'is': 588, 'it': 589, 'its': 590, 'itself': 591, 'jardiniere': 592, 'jealousy': 593, 'just': 594, 'keep': 595, 'kept': 596, 'kind': 597, 'knees': 598, 'knew': 599, 'know': 600, 'known': 601, 'laid': 602, 'lair': 603, 'landing': 604, 'language': 605, 'last': 606, 'late': 607, 'later': 608, 'latter': 609, 'laugh': 610, 'laughed': 611, 'lay': 612, 'leading': 613, 'lean': 614, 'learned': 615, 'least': 616, 'leathery': 617, 'leave': 618, 'led': 619, 'left': 620, 'leisure': 621, 'lends': 622, 'lent': 623, 'let': 624, 'lies': 625, 'life': 626, 'lift': 627, 'lifted': 628, 'light': 629, 'lightly': 630, 'like': 631, 'liked': 632, 'likeness': 633, 'line': 634, 'lines': 635, 'lingered': 636, 'lips': 637, 'lit': 638, 'little': 639, 'live': 640, 'll': 641, 'loathing': 642, 'long': 643, 'longed': 644, 'longer': 645, 'look': 646, 'looked': 647, 'looking': 648, 'lose': 649, 'loss': 650, 'lounging': 651, 'lovely': 652, 'lucky': 653, 'lump': 654, 'luncheon': 655, 'luxury': 656, 'lying': 657, 'made': 658, 'make': 659, 'man': 660, 'manage': 661, 'managed': 662, 'mantel': 663, 'marble': 664, 'married': 665, 'may': 666, 'me': 667, 'meant': 668, 'mechanically': 669, 'mediocrity': 670, 'medium': 671, 'mentioned': 672, 'mere': 673, 'merely': 674, 'met': 675, 'might': 676, 'mighty': 677, 'millionaire': 678, 'mine': 679, 'minute': 680, 'minutes': 681, 'mirrors': 682, 'modest': 683, 'modesty': 684, 'moment': 685, 'money': 686, 'monumental': 687, 'mood': 688, 'morbidly': 689, 'more': 690, 'most': 691, 'mourn': 692, 'mourned': 693, 'moustache': 694, 'moved': 695, 'much': 696, 'muddling': 697, 'multiplied': 698, 'murmur': 699, 'muscles': 700, 'must': 701, 'my': 702, 'myself': 703, 'mysterious': 704, 'naive': 705, 'near': 706, 'nearly': 707, 'negatived': 708, 'nervous': 709, 'nervousness': 710, 'neutral': 711, 'never': 712, 'next': 713, 'no': 714, 'none': 715, 'not': 716, 'note': 717, 'nothing': 718, 'now': 719, 'nymphs': 720, 'oak': 721, 'obituary': 722, 'object': 723, 'objects': 724, 'occurred': 725, 'oddly': 726, 'of': 727, 'off': 728, 'often': 729, 'oh': 730, 'old': 731, 'on': 732, 'once': 733, 'one': 734, 'ones': 735, 'only': 736, 'onto': 737, 'open': 738, 'or': 739, 'other': 740, 'our': 741, 'ourselves': 742, 'out': 743, 'outline': 744, 'oval': 745, 'over': 746, 'own': 747, 'packed': 748, 'paid': 749, 'paint': 750, 'painted': 751, 'painter': 752, 'painting': 753, 'pale': 754, 'paled': 755, 'palm': 756, 'panel': 757, 'panelled': 758, 'panelling': 759, 'pardonable': 760, 'pardoned': 761, 'part': 762, 'passages': 763, 'passing': 764, 'past': 765, 'pastels': 766, 'pathos': 767, 'patient': 768, 'people': 769, 'perceptible': 770, 'perfect': 771, 'persistence': 772, 'persuasively': 773, 'phrase': 774, 'picture': 775, 'pictures': 776, 'piece': 777, 'pines': 778, 'pink': 779, 'place': 780, 'placed': 781, 'plain': 782, 'platitudes': 783, 'pleased': 784, 'pockets': 785, 'point': 786, 'poised': 787, 'poor': 788, 'portrait': 789, 'posing': 790, 'possessed': 791, 'poverty': 792, 'predicted': 793, 'preliminary': 794, 'presenting': 795, 'presses': 796, 'prestidigitation': 797, 'pretty': 798, 'previous': 799, 'price': 800, 'pride': 801, 'princely': 802, 'prism': 803, 'problem': 804, 'proclaiming': 805, 'prodigious': 806, 'profusion': 807, 'protest': 808, 'prove': 809, 'public': 810, 'purblind': 811, 'purely': 812, 'pushed': 813, 'put': 814, 'qualities': 815, 'quality': 816, 'queerly': 817, 'question': 818, 'quickly': 819, 'quietly': 820, 'quite': 821, 'quote': 822, 'rain': 823, 'raised': 824, 'random': 825, 'rather': 826, 're': 827, 'real': 828, 'really': 829, 'reared': 830, 'reason': 831, 'reassurance': 832, 'recovering': 833, 'recreated': 834, 'reflected': 835, 'reflection': 836, 'regrets': 837, 'relatively': 838, 'remained': 839, 'remember': 840, 'reminded': 841, 'repeating': 842, 'represented': 843, 'reproduction': 844, 'resented': 845, 'resolve': 846, 'resources': 847, 'rest': 848, 'rich': 849, 'ridiculous': 850, 'robbed': 851, 'romantic': 852, 'room': 853, 'rooms': 854, 'rose': 855, 'rs': 856, 'rule': 857, 'run': 858, 's': 859, 'said': 860, 'same': 861, 'satisfaction': 862, 'savour': 863, 'saw': 864, 'say': 865, 'saying': 866, 'says': 867, 'scorn': 868, 'scornful': 869, 'secret': 870, 'see': 871, 'seemed': 872, 'seen': 873, 'self': 874, 'send': 875, 'sensation': 876, 'sensitive': 877, 'sent': 878, 'serious': 879, 'set': 880, 'sex': 881, 'shade': 882, 'shaking': 883, 'shall': 884, 'she': 885, 'shirked': 886, 'short': 887, 'should': 888, 'shoulder': 889, 'shoulders': 890, 'show': 891, 'showed': 892, 'showy': 893, 'shrug': 894, 'shrugged': 895, 'sight': 896, 'sign': 897, 'silent': 898, 'silver': 899, 'similar': 900, 'simpleton': 901, 'simplifications': 902, 'simply': 903, 'since': 904, 'single': 905, 'sitter': 906, 'sitters': 907, 'sketch': 908, 'skill': 909, 'slight': 910, 'slightly': 911, 'slowly': 912, 'small': 913, 'smile': 914, 'smiling': 915, 'sneer': 916, 'so': 917, 'solace': 918, 'some': 919, 'somebody': 920, 'something': 921, 'spacious': 922, 'spaniel': 923, 'speaking': 924, 'speculations': 925, 'spite': 926, 'splash': 927, 'square': 928, 'stairs': 929, 'stalk': 930, 'stammer': 931, 'stand': 932, 'standing': 933, 'started': 934, 'stay': 935, 'still': 936, 'stocked': 937, 'stood': 938, 'stopped': 939, 'stopping': 940, 'straddling': 941, 'straight': 942, 'strain': 943, 'straining': 944, 'strange': 945, 'straw': 946, 'stream': 947, 'stroke': 948, 'strokes': 949, 'strolled': 950, 'strongest': 951, 'strongly': 952, 'struck': 953, 'studio': 954, 'stuff': 955, 'subject': 956, 'substantial': 957, 'suburban': 958, 'such': 959, 'suddenly': 960, 'suffered': 961, 'sugar': 962, 'suggested': 963, 'sunburn': 964, 'sunburnt': 965, 'sunlit': 966, 'superb': 967, 'sure': 968, 'surest': 969, 'surface': 970, 'surprise': 971, 'surprised': 972, 'surrounded': 973, 'suspected': 974, 'sweetly': 975, 'sweetness': 976, 'swelling': 977, 'swept': 978, 'swum': 979, 't': 980, 'table': 981, 'take': 982, 'taken': 983, 'talking': 984, 'tea': 985, 'tears': 986, 'technicalities': 987, 'technique': 988, 'tell': 989, 'tells': 990, 'tempting': 991, 'terra': 992, 'terrace': 993, 'terraces': 994, 'terribly': 995, 'than': 996, 'that': 997, 'the': 998, 'their': 999, 'them': 1000, 'then': 1001, 'there': 1002, 'therefore': 1003, 'they': 1004, 'thin': 1005, 'thing': 1006, 'things': 1007, 'think': 1008, 'this': 1009, 'thither': 1010, 'those': 1011, 'though': 1012, 'thought': 1013, 'three': 1014, 'threshold': 1015, 'threw': 1016, 'through': 1017, 'throwing': 1018, 'tie': 1019, 'till': 1020, 'time': 1021, 'timorously': 1022, 'tinge': 1023, 'tips': 1024, 'tired': 1025, 'to': 1026, 'told': 1027, 'tone': 1028, 'tones': 1029, 'too': 1030, 'took': 1031, 'tottering': 1032, 'touched': 1033, 'toward': 1034, 'trace': 1035, 'trade': 1036, 'transmute': 1037, 'traps': 1038, 'travelled': 1039, 'trees': 1040, 'tribute': 1041, 'tributes': 1042, 'tricks': 1043, 'tried': 1044, 'trouser': 1045, 'true': 1046, 'truth': 1047, 'tubes': 1048, 'turned': 1049, 'twenty': 1050, 'twice': 1051, 'twirling': 1052, 'unaccountable': 1053, 'uncertain': 1054, 'under': 1055, 'underlay': 1056, 'underneath': 1057, 'understand': 1058, 'unexpected': 1059, 'untouched': 1060, 'unusual': 1061, 'up': 1062, 'upon': 1063, 'upset': 1064, 'upstairs': 1065, 'us': 1066, 'used': 1067, 'usual': 1068, 'value': 1069, 'varnishing': 1070, 'vases': 1071, 've': 1072, 'veins': 1073, 'velveteen': 1074, 'verte': 1075, 'very': 1076, 'villa': 1077, 'vindicated': 1078, 'virtuosity': 1079, 'vista': 1080, 'vocation': 1081, 'voice': 1082, 'wall': 1083, 'wander': 1084, 'want': 1085, 'wanted': 1086, 'wants': 1087, 'was': 1088, 'wasn': 1089, 'watched': 1090, 'watching': 1091, 'water': 1092, 'waves': 1093, 'way': 1094, 'weekly': 1095, 'weeks': 1096, 'welcome': 1097, 'went': 1098, 'were': 1099, 'what': 1100, 'when': 1101, 'whenever': 1102, 'where': 1103, 'which': 1104, 'while': 1105, 'white': 1106, 'who': 1107, 'whole': 1108, 'whom': 1109, 'why': 1110, 'wide': 1111, 'widow': 1112, 'wife': 1113, 'wild': 1114, 'wincing': 1115, 'window': 1116, 'wish': 1117, 'with': 1118, 'without': 1119, 'wits': 1120, 'woman': 1121, 'women': 1122, 'won': 1123, 'wonder': 1124, 'wondered': 1125, 'word': 1126, 'work': 1127, 'working': 1128, 'worth': 1129, 'would': 1130, 'wouldn': 1131, 'year': 1132, 'years': 1133, 'yellow': 1134, 'yet': 1135, 'you': 1136, 'younger': 1137, 'your': 1138, 'yourself': 1139}\n"
     ]
    }
   ],
   "source": [
    "vocab={token:integer for integer,token in enumerate(all_words)}\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "268c0c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'!': 0, '\"': 1, \"'\": 2, '(': 3, ')': 4, ',': 5, '-': 6, '--': 7, '.': 8, ':': 9, ';': 10, '?': 11, 'A': 12, 'Ah': 13, 'Among': 14, 'And': 15, 'Are': 16, 'Arrt': 17, 'As': 18, 'At': 19, 'Be': 20, 'Begin': 21, 'Burlington': 22, 'But': 23, 'By': 24, 'Carlo': 25, 'Chicago': 26, 'Claude': 27, 'Come': 28, 'Croft': 29, 'Destroyed': 30, 'Devonshire': 31, 'Don': 32, 'Dubarry': 33, 'Emperors': 34, 'Florence': 35, 'For': 36, 'Gallery': 37, 'Gideon': 38, 'Gisburn': 39, 'Gisburns': 40, 'Grafton': 41, 'Greek': 42, 'Grindle': 43, 'Grindles': 44, 'HAD': 45, 'Had': 46, 'Hang': 47, 'Has': 48, 'He': 49}\n"
     ]
    }
   ],
   "source": [
    "print(dict(itertools.islice(vocab.items(), 50)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3e0cfc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        \"\"\"\n",
    "        Initialize tokenizer with vocabulary.\n",
    "        vocab: dictionary mapping tokens to integers\n",
    "        \"\"\"\n",
    "        self.vocab = vocab\n",
    "        self.inverse_vocab = {v: k for k, v in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        \"\"\"\n",
    "        Encode text to token IDs.\n",
    "        Returns: dict with 'token_ids' (list of IDs in order), 'tokens' (list of words), \n",
    "        and 'mapper_dict' (token→ID mapping). Tokens not in vocab use null_character.\n",
    "        \"\"\"\n",
    "        pattern = r'(--|[,\\.\\-?!_;:\\'\"(){}\\[\\]<>/\\\\@#$%^&*+=]|\\s)'\n",
    "        result = re.split(pattern, text)\n",
    "        \n",
    "        # Clean tokens: remove whitespace-only and strip\n",
    "        tokens = [item.strip() for item in result if item.strip()]\n",
    "        \n",
    "        # Convert to IDs in order, use -1 for unknown tokens\n",
    "        token_ids = []\n",
    "        mapper_dict = {}\n",
    "        for token in tokens:\n",
    "            if token in self.vocab:\n",
    "                token_id = self.vocab[token]\n",
    "                token_ids.append(token_id)\n",
    "                mapper_dict[token] = token_id\n",
    "            else:\n",
    "                token_ids.append(-1)  # null_character for unknown tokens\n",
    "        \n",
    "        return {\n",
    "            'token_ids': token_ids,\n",
    "            'tokens': tokens,\n",
    "            'mapper_dict': mapper_dict\n",
    "        }\n",
    "    \n",
    "    def decode(self, token_ids):\n",
    "        \"\"\"\n",
    "        Decode token IDs back to text.\n",
    "        Takes list of token IDs (from encoder).\n",
    "        Returns: dict with 'tokens' (list of words), 'text' (joined sentence), \n",
    "        and 'mapper_dict' (ID→token mapping)\n",
    "        \"\"\"\n",
    "        tokens = []\n",
    "        mapper_dict = {}\n",
    "        \n",
    "        for token_id in token_ids:\n",
    "            if token_id == -1:\n",
    "                tokens.append('|<unk>|')  # unknown token\n",
    "            elif token_id in self.inverse_vocab:\n",
    "                token = self.inverse_vocab[token_id]\n",
    "                tokens.append(token)\n",
    "                mapper_dict[token_id] = token\n",
    "        \n",
    "        return {\n",
    "            'tokens': tokens,\n",
    "            'text': ' '.join(tokens),\n",
    "            'mapper_dict': mapper_dict\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "27304193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example:\n",
    "tokenizer = SimpleTokenizerV1(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "72845ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded IDs: [54, 45, 150, 1013, 58, 39, 826, 116, 259, 493, 7, 1012, 116, 507, 441, 398, 7, 917, 589, 1088, 714, 513, 971, 1026, 667, 1026, 538, 997, 5, 572, 998, 541, 727, 552, 503, 5, 536, 519, 376, 552, 753, 5, 665, 116, 849, 1112, 5, 158, 403, 550, 572, 116, 1077, 732, 998, 85, 8, 3, 100, 54, 826, 1013, 589, 1130, 533, 207, 86, 739, 35, 8, 4]\n"
     ]
    }
   ],
   "source": [
    "# Encode\n",
    "text = \"I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a rich widow, and established himself in a villa on the Riviera. (Though I rather thought it would have been Rome or Florence.)\"\n",
    "encoded = tokenizer.encode(text)\n",
    "# print(\"Encoded tokens:\", encoded['tokens'])\n",
    "print(\"Encoded IDs:\", encoded['token_ids'])\n",
    "# print(\"Token→ID Mapper:\", encoded['mapper_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ef556f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded text: I HAD always thought Jack Gisburn rather a cheap genius -- though a good fellow enough -- so it was no great surprise to me to hear that , in the height of his glory , he had dropped his painting , married a rich widow , and established himself in a villa on the Riviera . ( Though I rather thought it would have been Rome or Florence . )\n"
     ]
    }
   ],
   "source": [
    "decoded = tokenizer.decode(encoded['token_ids'])\n",
    "# print(\"\\nDecoded tokens:\", decoded['tokens'])\n",
    "print(\"Decoded text:\", decoded['text'])\n",
    "# print(\"ID→Token Mapper:\", decoded['mapper_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6423d3e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'|<unk>| |<unk>| ! , How are you ?'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(\"Hello world!, How are you?\")['token_ids'])['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9f2ffb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modifying The Vocabulary to Indicate Unknown Tokens and End of Text\n",
    "\n",
    "all_words.extend(['|<unk>|', '|<endoftext>|'])\n",
    "\n",
    "vocab_updated={token:integer for integer,token in enumerate(all_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "72410ec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'year': 1132,\n",
       " 'years': 1133,\n",
       " 'yellow': 1134,\n",
       " 'yet': 1135,\n",
       " 'you': 1136,\n",
       " 'younger': 1137,\n",
       " 'your': 1138,\n",
       " 'yourself': 1139,\n",
       " '|<unk>|': 1140,\n",
       " '|<endoftext>|': 1141}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(list(vocab_updated.items())[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eaba7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        \"\"\"\n",
    "        Initialize tokenizer with vocabulary.\n",
    "        vocab: dictionary mapping tokens to integers\n",
    "        \"\"\"\n",
    "        self.vocab = vocab\n",
    "        self.inverse_vocab = {v: k for k, v in vocab.items()}\n",
    "        # Get special token IDs\n",
    "        self.unk_token_id = vocab.get('|<unk>|', -1)\n",
    "        self.endoftext_token_id = vocab.get('|<endoftext>|', -1)\n",
    "    \n",
    "    def encode(self, text):\n",
    "        \"\"\"\n",
    "        Encode text to token IDs.\n",
    "        Uses vocab_updated with unknown and endoftext tokens.\n",
    "        Returns: dict with 'token_ids' (list of IDs in order), 'tokens' (list of words), \n",
    "        and 'mapper_dict' (token→ID mapping). Unknown tokens use |<unk>| token ID.\n",
    "        \"\"\"\n",
    "        pattern = r'(--|[,\\.\\-?!_;:\\'\"(){}\\[\\]<>/\\\\@#$%^&*+=]|\\s)'\n",
    "        result = re.split(pattern, text)\n",
    "        \n",
    "        # Clean tokens: remove whitespace-only and strip\n",
    "        tokens = [item.strip() for item in result if item.strip()]\n",
    "        \n",
    "        # Convert to IDs in order, use |<unk>| token ID for unknown tokens\n",
    "        token_ids = []\n",
    "        mapper_dict = {}\n",
    "        for token in tokens:\n",
    "            if token in self.vocab:\n",
    "                token_id = self.vocab[token]\n",
    "                token_ids.append(token_id)\n",
    "                mapper_dict[token] = token_id\n",
    "            else:\n",
    "                # Use the unknown token ID instead of -1\n",
    "                token_ids.append(self.unk_token_id)\n",
    "                mapper_dict[token] = self.unk_token_id\n",
    "        \n",
    "        # Add end-of-text token at the end\n",
    "        token_ids.append(self.endoftext_token_id)\n",
    "        \n",
    "        return {\n",
    "            'token_ids': token_ids,\n",
    "            'tokens': tokens,\n",
    "            'mapper_dict': mapper_dict\n",
    "        }\n",
    "    \n",
    "    def decode(self, token_ids):\n",
    "        \"\"\"\n",
    "        Decode token IDs back to text.\n",
    "        Takes list of token IDs (from encoder).\n",
    "        Returns: dict with 'tokens' (list of words), 'text' (joined sentence), \n",
    "        and 'mapper_dict' (ID→token mapping). Includes |<endoftext>| token.\n",
    "        \"\"\"\n",
    "        tokens = []\n",
    "        mapper_dict = {}\n",
    "        \n",
    "        for token_id in token_ids:\n",
    "            if token_id in self.inverse_vocab:\n",
    "                token = self.inverse_vocab[token_id]\n",
    "                tokens.append(token)\n",
    "                mapper_dict[token_id] = token\n",
    "        \n",
    "        return {\n",
    "            'tokens': tokens,\n",
    "            'text': ' '.join(tokens),\n",
    "            'mapper_dict': mapper_dict\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b67f6e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: Using SimpleTokenizerV2 with vocab_updated\n",
    "tokenizer_v2 = SimpleTokenizerV2(vocab_updated)\n",
    "\n",
    "# Test sentence\n",
    "test_text = \"Hello world!, How are you?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d2f741fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Encoded IDs (first 20): [1140, 1140, 0, 5, 53, 170, 1136, 11, 1141]\n"
     ]
    }
   ],
   "source": [
    "# Encode\n",
    "encoded_v2 = tokenizer_v2.encode(test_text)\n",
    "\n",
    "# print(\"\\nEncoded tokens (first 20):\", encoded_v2['tokens'][:20])\n",
    "print(\"\\nEncoded IDs (first 20):\", encoded_v2['token_ids'])\n",
    "# print(f\"\\nTotal token IDs: {len(encoded_v2['token_ids'])}\")\n",
    "# print(f\"Unknown token ID (|<unk>|): {tokenizer_v2.unk_token_id}\")\n",
    "# print(f\"End-of-text token ID (|<endoftext>|): {tokenizer_v2.endoftext_token_id}\")\n",
    "# print(f\"Last ID in token_ids: {encoded_v2['token_ids'][-1]} (should be endoftext token)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f053e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decoded text:\n",
      "|<unk>| |<unk>| ! , How are you ?\n"
     ]
    }
   ],
   "source": [
    "# Decode\n",
    "decoded_v2 = tokenizer_v2.decode(encoded_v2['token_ids'])\n",
    "print(\"\\nDecoded tokens:\")\n",
    "print(decoded_v2['tokens'])\n",
    "print(\"\\nDecoded text:\")\n",
    "print(decoded_v2['text'])\n",
    "print(\"\\nMapper dict:\")\n",
    "print(decoded_v2['mapper_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38d0280",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_from_scratch (3.10.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
