{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0a8dcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d823d3",
   "metadata": {},
   "source": [
    "#### Setence taken `Your journey starts with one step` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d1a8de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([\n",
    "    [ 0.43, 0.15,  0.89],\n",
    "    [0.55,  0.87,  0.66],\n",
    "    [ 0.57,  0.85, 0.64],\n",
    "    [0.22, 0.58,  0.33],\n",
    "    [ 0.77,  0.25, 0.10],\n",
    "    [ 0.05,  0.80,  0.55]\n",
    "], dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2d2920f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4300, 0.1500, 0.8900],\n",
       "        [0.5500, 0.8700, 0.6600],\n",
       "        [0.5700, 0.8500, 0.6400],\n",
       "        [0.2200, 0.5800, 0.3300],\n",
       "        [0.7700, 0.2500, 0.1000],\n",
       "        [0.0500, 0.8000, 0.5500]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05574bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = inputs[1]  # Second \n",
    "\n",
    "attn_scores_2=torch.empty(inputs.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f20d14d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,x_i in enumerate(inputs):\n",
    "    attn_scores_2[i] = torch.dot(query, x_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4bfaef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: tensor([0.5500, 0.8700, 0.6600]) x_i tensor([0.4300, 0.1500, 0.8900])\n",
      "query: tensor([0.5500, 0.8700, 0.6600]) x_i tensor([0.5500, 0.8700, 0.6600])\n",
      "query: tensor([0.5500, 0.8700, 0.6600]) x_i tensor([0.5700, 0.8500, 0.6400])\n",
      "query: tensor([0.5500, 0.8700, 0.6600]) x_i tensor([0.2200, 0.5800, 0.3300])\n",
      "query: tensor([0.5500, 0.8700, 0.6600]) x_i tensor([0.7700, 0.2500, 0.1000])\n",
      "query: tensor([0.5500, 0.8700, 0.6600]) x_i tensor([0.0500, 0.8000, 0.5500])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.4950)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i,x_i in enumerate(inputs):\n",
    "    print(\"query:\",query,'x_i',x_i)\n",
    "\n",
    "\n",
    "torch.dot(query,inputs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2125ee2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_scores_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9085296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n",
      "tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n"
     ]
    }
   ],
   "source": [
    "normalized_attn_scores_2 = attn_scores_2 / attn_scores_2.sum()\n",
    "\n",
    "\n",
    "print(attn_scores_2)\n",
    "print(normalized_attn_scores_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f885b586",
   "metadata": {},
   "source": [
    "### Softmax Implementation for Attention Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52e2c2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_softmax(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb0d84ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "attn_scores_2_native_softmax = naive_softmax(attn_scores_2)\n",
    "\n",
    "print(attn_scores_2_native_softmax)\n",
    "print(attn_scores_2_native_softmax.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27d1baab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "attn_scores_2_softmax_torch = torch.softmax(attn_scores_2, dim=0)\n",
    "\n",
    "print(attn_scores_2_softmax_torch)\n",
    "print(attn_scores_2_softmax_torch.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2a34e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "context_vector_2 = torch.zeros(query.shape)\n",
    "\n",
    "for i,x_i in enumerate(inputs):\n",
    "    context_vector_2 += attn_scores_2_softmax_torch[i] * x_i\n",
    "\n",
    "print(context_vector_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02735fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_scores=torch.empty(6,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "584cef61",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_scores = inputs @ inputs.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e55428c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4300, 0.1500, 0.8900])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e7204d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
       "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
       "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
       "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
       "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
       "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d634075f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
       "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
       "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
       "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
       "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
       "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights=torch.softmax(attn_scores,dim=-1)\n",
    "attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ca11c83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1981, 0.2333, 0.2326, 0.2046, 0.1975, 0.2128])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "33843ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_matrix=attn_weights @ inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4baadcae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4300, 0.1500, 0.8900],\n",
       "        [0.5500, 0.8700, 0.6600],\n",
       "        [0.5700, 0.8500, 0.6400],\n",
       "        [0.2200, 0.5800, 0.3300],\n",
       "        [0.7700, 0.2500, 0.1000],\n",
       "        [0.0500, 0.8000, 0.5500]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7913c412",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4421, 0.5931, 0.5790],\n",
       "        [0.4419, 0.6515, 0.5683],\n",
       "        [0.4431, 0.6496, 0.5671],\n",
       "        [0.4304, 0.6298, 0.5510],\n",
       "        [0.4671, 0.5910, 0.5266],\n",
       "        [0.4177, 0.6503, 0.5645]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e41adf71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4300, 0.1500, 0.8900],\n",
       "        [0.5500, 0.8700, 0.6600],\n",
       "        [0.5700, 0.8500, 0.6400],\n",
       "        [0.2200, 0.5800, 0.3300],\n",
       "        [0.7700, 0.2500, 0.1000],\n",
       "        [0.0500, 0.8000, 0.5500]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ab70c6",
   "metadata": {},
   "source": [
    "## Self Attention With Traianble Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c7ad153e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4300, 0.1500, 0.8900],\n",
       "        [0.5500, 0.8700, 0.6600],\n",
       "        [0.5700, 0.8500, 0.6400],\n",
       "        [0.2200, 0.5800, 0.3300],\n",
       "        [0.7700, 0.2500, 0.1000],\n",
       "        [0.0500, 0.8000, 0.5500]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "31ceecaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2=inputs[1]\n",
    "d_in=inputs.shape[1]\n",
    "d_out=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "72b01e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "W_query=torch.nn.Parameter(torch.rand(d_in,d_out),requires_grad=False)\n",
    "W_key=torch.nn.Parameter(torch.rand(d_in,d_out),requires_grad=False)\n",
    "W_value=torch.nn.Parameter(torch.rand(d_in,d_out),requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7204a2a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.0756, 0.1966],\n",
       "        [0.3164, 0.4017],\n",
       "        [0.1186, 0.8274]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ff6b9ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "query=inputs@W_query\n",
    "key=inputs@W_key\n",
    "value=inputs@W_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b51398f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2309, 1.0966],\n",
       "        [0.4306, 1.4551],\n",
       "        [0.4300, 1.4343],\n",
       "        [0.2355, 0.7990],\n",
       "        [0.2983, 0.6565],\n",
       "        [0.2568, 1.0533]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cbfe70e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "QK=query@key.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "41daef08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9231, 1.3545, 1.3241, 0.7910, 0.4032, 1.1330],\n",
       "        [1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440],\n",
       "        [1.2544, 1.8284, 1.7877, 1.0654, 0.5508, 1.5238],\n",
       "        [0.6973, 1.0167, 0.9941, 0.5925, 0.3061, 0.8475],\n",
       "        [0.6114, 0.8819, 0.8626, 0.5121, 0.2707, 0.7307],\n",
       "        [0.8995, 1.3165, 1.2871, 0.7682, 0.3937, 1.0996]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a6ed4a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_k=key.shape[-1]\n",
    "attn_weights_2=torch.softmax(QK/d_k**0.5,dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6a7a1dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "V=inputs@W_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7b0f4990",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_attn_weights=attn_weights@V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d3526b66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2897, 0.8043],\n",
       "        [0.3069, 0.8188],\n",
       "        [0.3063, 0.8173],\n",
       "        [0.2972, 0.7936],\n",
       "        [0.2848, 0.7650],\n",
       "        [0.3043, 0.8105]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "94a835a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention_v1(nn.Module):\n",
    "    \"\"\"\n",
    "    A single-head self-attention mechanism.\n",
    "\n",
    "    This module projects the input into query, key, and value vectors\n",
    "    and then computes the attention weights and context vectors.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_in: int, d_out: int):\n",
    "        \"\"\"\n",
    "        Initializes the SelfAttention_v1 module.\n",
    "\n",
    "        Args:\n",
    "            d_in (int): The dimensionality of the input feature vector.\n",
    "            d_out (int): The dimensionality of the output query, key, and value vectors.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Performs the forward pass of the self-attention mechanism.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor of shape (batch_size, num_tokens, d_in).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The context vector of shape (batch_size, num_tokens, d_out).\n",
    "        \"\"\"\n",
    "        # Project inputs into query, key, and value vectors\n",
    "        queries = self.W_query(x)\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # Compute attention scores\n",
    "        # The last dimension of keys is d_out, so we transpose the last two dimensions\n",
    "        attn_scores = queries @ keys.T\n",
    "\n",
    "        # Scale scores and apply softmax to get weights\n",
    "        # The scaling factor is the square root of the key dimension\n",
    "        attn_weights = torch.softmax(attn_scores / self.d_out**0.5, dim=-1)\n",
    "\n",
    "        # Compute the context vectors by weighting the values\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "62c32e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(200)\n",
    "attention_v1=SelfAttention_v1(d_in,d_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b4761995",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2789, 0.2778],\n",
       "        [0.2744, 0.2670],\n",
       "        [0.2743, 0.2666],\n",
       "        [0.2742, 0.2664],\n",
       "        [0.2712, 0.2592],\n",
       "        [0.2758, 0.2704]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_v1(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04d3ff9",
   "metadata": {},
   "source": [
    "## Causal Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a45a8990",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2440392054.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    testing code\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "testing code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3394d845",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58403926",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be920a40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bfa857",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2440849",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_from_scratch (3.10.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
