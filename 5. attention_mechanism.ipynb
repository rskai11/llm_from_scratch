{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0a8dcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d823d3",
   "metadata": {},
   "source": [
    "#### Setence taken `Your journey starts with one step` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d1a8de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([\n",
    "    [ 0.43, 0.15,  0.89],\n",
    "    [0.55,  0.87,  0.66],\n",
    "    [ 0.57,  0.85, 0.64],\n",
    "    [0.22, 0.58,  0.33],\n",
    "    [ 0.77,  0.25, 0.10],\n",
    "    [ 0.05,  0.80,  0.55]\n",
    "], dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2d2920f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4300, 0.1500, 0.8900],\n",
       "        [0.5500, 0.8700, 0.6600],\n",
       "        [0.5700, 0.8500, 0.6400],\n",
       "        [0.2200, 0.5800, 0.3300],\n",
       "        [0.7700, 0.2500, 0.1000],\n",
       "        [0.0500, 0.8000, 0.5500]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05574bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = inputs[1]  # Second \n",
    "\n",
    "attn_scores_2=torch.empty(inputs.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f20d14d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,x_i in enumerate(inputs):\n",
    "    attn_scores_2[i] = torch.dot(query, x_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4bfaef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: tensor([0.5500, 0.8700, 0.6600]) x_i tensor([0.4300, 0.1500, 0.8900])\n",
      "query: tensor([0.5500, 0.8700, 0.6600]) x_i tensor([0.5500, 0.8700, 0.6600])\n",
      "query: tensor([0.5500, 0.8700, 0.6600]) x_i tensor([0.5700, 0.8500, 0.6400])\n",
      "query: tensor([0.5500, 0.8700, 0.6600]) x_i tensor([0.2200, 0.5800, 0.3300])\n",
      "query: tensor([0.5500, 0.8700, 0.6600]) x_i tensor([0.7700, 0.2500, 0.1000])\n",
      "query: tensor([0.5500, 0.8700, 0.6600]) x_i tensor([0.0500, 0.8000, 0.5500])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.4950)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i,x_i in enumerate(inputs):\n",
    "    print(\"query:\",query,'x_i',x_i)\n",
    "\n",
    "\n",
    "torch.dot(query,inputs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2125ee2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_scores_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9085296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n",
      "tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n"
     ]
    }
   ],
   "source": [
    "normalized_attn_scores_2 = attn_scores_2 / attn_scores_2.sum()\n",
    "\n",
    "\n",
    "print(attn_scores_2)\n",
    "print(normalized_attn_scores_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f885b586",
   "metadata": {},
   "source": [
    "### Softmax Implementation for Attention Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52e2c2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_softmax(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb0d84ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "attn_scores_2_native_softmax = naive_softmax(attn_scores_2)\n",
    "\n",
    "print(attn_scores_2_native_softmax)\n",
    "print(attn_scores_2_native_softmax.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27d1baab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "attn_scores_2_softmax_torch = torch.softmax(attn_scores_2, dim=0)\n",
    "\n",
    "print(attn_scores_2_softmax_torch)\n",
    "print(attn_scores_2_softmax_torch.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2a34e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "context_vector_2 = torch.zeros(query.shape)\n",
    "\n",
    "for i,x_i in enumerate(inputs):\n",
    "    context_vector_2 += attn_scores_2_softmax_torch[i] * x_i\n",
    "\n",
    "print(context_vector_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02735fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_scores=torch.empty(6,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "584cef61",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_scores = inputs @ inputs.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e55428c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4300, 0.1500, 0.8900])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e7204d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
       "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
       "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
       "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
       "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
       "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d634075f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
       "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
       "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
       "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
       "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
       "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights=torch.softmax(attn_scores,dim=-1)\n",
    "attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ca11c83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1981, 0.2333, 0.2326, 0.2046, 0.1975, 0.2128])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "33843ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_matrix=attn_weights @ inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4baadcae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4300, 0.1500, 0.8900],\n",
       "        [0.5500, 0.8700, 0.6600],\n",
       "        [0.5700, 0.8500, 0.6400],\n",
       "        [0.2200, 0.5800, 0.3300],\n",
       "        [0.7700, 0.2500, 0.1000],\n",
       "        [0.0500, 0.8000, 0.5500]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7913c412",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4421, 0.5931, 0.5790],\n",
       "        [0.4419, 0.6515, 0.5683],\n",
       "        [0.4431, 0.6496, 0.5671],\n",
       "        [0.4304, 0.6298, 0.5510],\n",
       "        [0.4671, 0.5910, 0.5266],\n",
       "        [0.4177, 0.6503, 0.5645]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e41adf71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4300, 0.1500, 0.8900],\n",
       "        [0.5500, 0.8700, 0.6600],\n",
       "        [0.5700, 0.8500, 0.6400],\n",
       "        [0.2200, 0.5800, 0.3300],\n",
       "        [0.7700, 0.2500, 0.1000],\n",
       "        [0.0500, 0.8000, 0.5500]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ab70c6",
   "metadata": {},
   "source": [
    "## Self Attention With Traianble Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c7ad153e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4300, 0.1500, 0.8900],\n",
       "        [0.5500, 0.8700, 0.6600],\n",
       "        [0.5700, 0.8500, 0.6400],\n",
       "        [0.2200, 0.5800, 0.3300],\n",
       "        [0.7700, 0.2500, 0.1000],\n",
       "        [0.0500, 0.8000, 0.5500]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "31ceecaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2=inputs[1]\n",
    "d_in=inputs.shape[1]\n",
    "d_out=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "72b01e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "W_query=torch.nn.Parameter(torch.rand(d_in,d_out),requires_grad=False)\n",
    "W_key=torch.nn.Parameter(torch.rand(d_in,d_out),requires_grad=False)\n",
    "W_value=torch.nn.Parameter(torch.rand(d_in,d_out),requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7204a2a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.0756, 0.1966],\n",
       "        [0.3164, 0.4017],\n",
       "        [0.1186, 0.8274]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ff6b9ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "query=inputs@W_query\n",
    "key=inputs@W_key\n",
    "value=inputs@W_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b51398f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2309, 1.0966],\n",
       "        [0.4306, 1.4551],\n",
       "        [0.4300, 1.4343],\n",
       "        [0.2355, 0.7990],\n",
       "        [0.2983, 0.6565],\n",
       "        [0.2568, 1.0533]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cbfe70e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "QK=query@key.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "41daef08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9231, 1.3545, 1.3241, 0.7910, 0.4032, 1.1330],\n",
       "        [1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440],\n",
       "        [1.2544, 1.8284, 1.7877, 1.0654, 0.5508, 1.5238],\n",
       "        [0.6973, 1.0167, 0.9941, 0.5925, 0.3061, 0.8475],\n",
       "        [0.6114, 0.8819, 0.8626, 0.5121, 0.2707, 0.7307],\n",
       "        [0.8995, 1.3165, 1.2871, 0.7682, 0.3937, 1.0996]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a6ed4a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_k=key.shape[-1]\n",
    "attn_weights_2=torch.softmax(QK/d_k**0.5,dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6a7a1dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "V=inputs@W_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7b0f4990",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_attn_weights=attn_weights@V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d3526b66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2897, 0.8043],\n",
       "        [0.3069, 0.8188],\n",
       "        [0.3063, 0.8173],\n",
       "        [0.2972, 0.7936],\n",
       "        [0.2848, 0.7650],\n",
       "        [0.3043, 0.8105]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "94a835a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention_v1(nn.Module):\n",
    "    \"\"\"\n",
    "    A single-head self-attention mechanism.\n",
    "\n",
    "    This module projects the input into query, key, and value vectors\n",
    "    and then computes the attention weights and context vectors.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_in: int, d_out: int):\n",
    "        \"\"\"\n",
    "        Initializes the SelfAttention_v1 module.\n",
    "\n",
    "        Args:\n",
    "            d_in (int): The dimensionality of the input feature vector.\n",
    "            d_out (int): The dimensionality of the output query, key, and value vectors.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Performs the forward pass of the self-attention mechanism.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor of shape (batch_size, num_tokens, d_in).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The context vector of shape (batch_size, num_tokens, d_out).\n",
    "        \"\"\"\n",
    "        # Project inputs into query, key, and value vectors\n",
    "        queries = self.W_query(x)\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # Compute attention scores\n",
    "        # The last dimension of keys is d_out, so we transpose the last two dimensions\n",
    "        attn_scores = queries @ keys.T\n",
    "\n",
    "        # Scale scores and apply softmax to get weights\n",
    "        # The scaling factor is the square root of the key dimension\n",
    "        attn_weights = torch.softmax(attn_scores / self.d_out**0.5, dim=-1)\n",
    "\n",
    "        # Compute the context vectors by weighting the values\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "62c32e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(200)\n",
    "attention_v1=SelfAttention_v1(d_in,d_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b4761995",
   "metadata": {},
   "outputs": [],
   "source": [
    "atten_weights_3=attention_v1(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04d3ff9",
   "metadata": {},
   "source": [
    "## Causal Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a45a8990",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length=atten_weights_3.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3394d845",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_simple=torch.tril(torch.ones(context_length,context_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "58403926",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "be920a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_attn=attn_weights_2*mask_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d7bfa857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1551, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1500, 0.2264, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1503, 0.2256, 0.2192, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1591, 0.1994, 0.1962, 0.1477, 0.0000, 0.0000],\n",
       "        [0.1610, 0.1949, 0.1923, 0.1501, 0.1265, 0.0000],\n",
       "        [0.1557, 0.2092, 0.2048, 0.1419, 0.1089, 0.1794]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d2440849",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_sums = masked_attn.sum(dim=1,keepdim=True)\n",
    "masked_simple_norm=masked_attn/rows_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d6d96df1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3986, 0.6014, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2526, 0.3791, 0.3683, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2265, 0.2839, 0.2794, 0.2103, 0.0000, 0.0000],\n",
       "        [0.1952, 0.2363, 0.2331, 0.1820, 0.1534, 0.0000],\n",
       "        [0.1557, 0.2092, 0.2048, 0.1419, 0.1089, 0.1794]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_simple_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7b3b4f7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1551],\n",
       "        [0.3764],\n",
       "        [0.5952],\n",
       "        [0.7024],\n",
       "        [0.8248],\n",
       "        [1.0000]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_attn.sum(dim=1,keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9c6141f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1551, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1500, 0.2264, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1503, 0.2256, 0.2192, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1591, 0.1994, 0.1962, 0.1477, 0.0000, 0.0000],\n",
       "        [0.1610, 0.1949, 0.1923, 0.1501, 0.1265, 0.0000],\n",
       "        [0.1557, 0.2092, 0.2048, 0.1419, 0.1089, 0.1794]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb59208",
   "metadata": {},
   "source": [
    "#### The above ones are not efficient so now will do the efficient way cause after doing softmax it creates an data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0040af83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1551, 0.2104, 0.2059, 0.1413, 0.1074, 0.1799],\n",
       "        [0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820],\n",
       "        [0.1503, 0.2256, 0.2192, 0.1315, 0.0914, 0.1819],\n",
       "        [0.1591, 0.1994, 0.1962, 0.1477, 0.1206, 0.1769],\n",
       "        [0.1610, 0.1949, 0.1923, 0.1501, 0.1265, 0.1752],\n",
       "        [0.1557, 0.2092, 0.2048, 0.1419, 0.1089, 0.1794]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "80eebb73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1551,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
       "        [0.1500, 0.2264,   -inf,   -inf,   -inf,   -inf],\n",
       "        [0.1503, 0.2256, 0.2192,   -inf,   -inf,   -inf],\n",
       "        [0.1591, 0.1994, 0.1962, 0.1477,   -inf,   -inf],\n",
       "        [0.1610, 0.1949, 0.1923, 0.1501, 0.1265,   -inf],\n",
       "        [0.1557, 0.2092, 0.2048, 0.1419, 0.1089, 0.1794]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask=torch.triu(torch.ones(context_length,context_length),diagonal=1)\n",
    "masked=attn_weights_2.masked_fill(mask.bool(),-torch.inf)\n",
    "masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9e22ef3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.4865, 0.5135, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3221, 0.3397, 0.3382, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2471, 0.2542, 0.2536, 0.2451, 0.0000, 0.0000],\n",
       "        [0.1994, 0.2042, 0.2039, 0.1979, 0.1946, 0.0000],\n",
       "        [0.1653, 0.1717, 0.1712, 0.1637, 0.1599, 0.1681]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atten_weights_4=torch.softmax(masked/key.shape[-1]**0.5,dim=1)\n",
    "atten_weights_4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ff464c",
   "metadata": {},
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "55b29d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 2.],\n",
      "        [2., 2., 0., 0., 2., 0.],\n",
      "        [0., 0., 2., 0., 0., 0.],\n",
      "        [2., 0., 2., 2., 0., 2.],\n",
      "        [2., 0., 0., 0., 0., 2.],\n",
      "        [2., 0., 2., 2., 2., 2.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(200)\n",
    "dropout=torch.nn.Dropout(0.5)\n",
    "\n",
    "\n",
    "## Demo Example How Dropout Work\n",
    "print(dropout(torch.ones(6,6)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "69f908be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.9730, 1.0270, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.6764, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4941, 0.0000, 0.5073, 0.4902, 0.0000, 0.0000],\n",
      "        [0.3988, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3307, 0.0000, 0.3423, 0.3275, 0.3199, 0.3362]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(200)\n",
    "\n",
    "print(dropout(atten_weights_4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bf4911",
   "metadata": {},
   "source": [
    "## Step,Operation,\"The Logical \"\"Why\"\"\"\n",
    "1. Q⋅KT,Calculate raw similarity scores.\n",
    "2. Scale (d​1​),Normalize the scores so gradients don't vanish.\n",
    "3. Mask,\"Apply the \"\"don't look ahead\"\" rule.\"\n",
    "4. Softmax,Convert scores to probabilities (0.0 to 1.0).\n",
    "5. Dropout,Randomly break connections to improve learning.\n",
    "6. ⋅V,Mix the content based on the probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4a85c502",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.register_buffer(\n",
    "            'mask',\n",
    "            torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        \n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # 1. Compute Raw Scores\n",
    "        attn_scores = queries @ keys.transpose(1, 2)\n",
    "\n",
    "        # 2. SCALE FIRST (The Correction)\n",
    "        # We scale the dot products down before masking.\n",
    "        # This prevents large numbers from exploding and ensures numerical stability.\n",
    "        attn_scores = attn_scores / self.d_out**0.5\n",
    "        \n",
    "        # 3. MASK SECOND (The Correction)\n",
    "        # We apply the mask to the already-scaled scores.\n",
    "        # This guarantees the mask value (even if finite) isn't accidentally shrunk.\n",
    "        attn_scores.masked_fill_(\n",
    "            self.mask[:num_tokens, :num_tokens].bool(), -torch.inf\n",
    "        )\n",
    "\n",
    "        # 4. Softmax\n",
    "        # Note: We removed the division here because we did it in Step 2.\n",
    "        attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "        \n",
    "        # 5. Dropout\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8163797b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch=torch.stack((inputs,inputs),dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "af7f6eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length=batch.shape[1]\n",
    "\n",
    "causal_attn_obj=CausalAttention(d_in=d_in,d_out=d_out, context_length=context_length, dropout=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2fd0c1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "causal_attn_weights=causal_attn_obj(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8202f84b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000,  0.0000],\n",
       "         [ 0.0983,  0.3780],\n",
       "         [-0.1334,  0.0204],\n",
       "         [-0.0083,  0.3134],\n",
       "         [ 0.0242,  0.3473],\n",
       "         [ 0.0187,  0.2463]],\n",
       "\n",
       "        [[ 0.0000,  0.0000],\n",
       "         [-0.1039,  0.4090],\n",
       "         [-0.0680,  0.2686],\n",
       "         [ 0.0935,  0.2976],\n",
       "         [ 0.0261,  0.4011],\n",
       "         [ 0.1180,  0.3631]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "causal_attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408a514b",
   "metadata": {},
   "source": [
    "## **Multi Head Attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "98f05a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.heads=nn.ModuleList(\n",
    "            [CausalAttention(d_in=d_in,d_out=d_out,context_length=context_length,dropout=dropout,qkv_bias=qkv_bias)\n",
    "              for _ in range(num_heads)]\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return torch.cat([head(x) for head in self.heads],dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c9ca2648",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length=batch.shape[1]\n",
    "d_in,d_out=3,2\n",
    "MHA=MultiHeadAttentionWrapper(d_in,d_out,context_length,0.0,num_heads=3)\n",
    "context_vecs=MHA(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b3ee5a11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063,  0.4566,  0.2729],\n",
       "         [-0.5874,  0.0058,  0.5891,  0.3257,  0.5792,  0.3011],\n",
       "         [-0.6300, -0.0632,  0.6202,  0.3860,  0.6249,  0.3102],\n",
       "         [-0.5675, -0.0843,  0.5478,  0.3589,  0.5691,  0.2785],\n",
       "         [-0.5526, -0.0981,  0.5321,  0.3428,  0.5543,  0.2520],\n",
       "         [-0.5299, -0.1081,  0.5077,  0.3493,  0.5337,  0.2499]],\n",
       "\n",
       "        [[-0.4519,  0.2216,  0.4772,  0.1063,  0.4566,  0.2729],\n",
       "         [-0.5874,  0.0058,  0.5891,  0.3257,  0.5792,  0.3011],\n",
       "         [-0.6300, -0.0632,  0.6202,  0.3860,  0.6249,  0.3102],\n",
       "         [-0.5675, -0.0843,  0.5478,  0.3589,  0.5691,  0.2785],\n",
       "         [-0.5526, -0.0981,  0.5321,  0.3428,  0.5543,  0.2520],\n",
       "         [-0.5299, -0.1081,  0.5077,  0.3493,  0.5337,  0.2499]]],\n",
       "       grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2c64ab36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6, 6])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vecs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f862a9",
   "metadata": {},
   "source": [
    "#### **IMPLEMENTING MULTIHEAD ATTENTION WITH WEIGHT SPLITS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "dd8b326b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "            \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                       diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) \n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        \n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2) \n",
    "        \n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec) # optional projection\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fd690e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 6])\n",
      "tensor([[[ 0.1569, -0.0873,  0.0210,  0.0215, -0.3243, -0.2518],\n",
      "         [ 0.1117, -0.0547,  0.0406, -0.0213, -0.3251, -0.2993],\n",
      "         [ 0.1196, -0.0491,  0.0318, -0.0635, -0.2788, -0.2578]],\n",
      "\n",
      "        [[ 0.1569, -0.0873,  0.0210,  0.0215, -0.3243, -0.2518],\n",
      "         [ 0.1117, -0.0547,  0.0406, -0.0213, -0.3251, -0.2993],\n",
      "         [ 0.1196, -0.0491,  0.0318, -0.0635, -0.2788, -0.2578]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 3, 6])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "# Define the tensor with 3 rows and 6 columns\n",
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.89, 0.55, 0.87, 0.66],  # Row 1\n",
    "     [0.57, 0.85, 0.64, 0.22, 0.58, 0.33],  # Row 2\n",
    "     [0.77, 0.25, 0.10, 0.05, 0.80, 0.55]]  # Row 3\n",
    ")\n",
    "\n",
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "print(batch.shape) \n",
    "\n",
    "batch_size, context_length, d_in = batch.shape\n",
    "d_out = 6\n",
    "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e2865a",
   "metadata": {},
   "source": [
    "## Actual Production Scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "84fcf312",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiHeadAttentionProd(nn.Module):\n",
    "    def __init__(self, d_model=768, n_head=12):\n",
    "        super().__init__()\n",
    "        self.n_head = n_head\n",
    "        self.d_model = d_model\n",
    "        self.head_dim = d_model // n_head\n",
    "        \n",
    "        # Ensure the dimensions are divisible\n",
    "        assert d_model % n_head == 0, \"Embedding length must be divisible by number of heads\"\n",
    "\n",
    "        # 1. The Projections (Linear layers)\n",
    "        # We create the Query, Key, and Value matrices\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # The final output layer after merging heads\n",
    "        self.fc_out = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [Batch_Size, Seq_Len, 768]\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # STEP 1: Linear Transformation\n",
    "        # Pass input through Q, K, V weights\n",
    "        Q = self.w_q(x)  # [Batch, Seq_Len, 768]\n",
    "        K = self.w_k(x)  # [Batch, Seq_Len, 768]\n",
    "        V = self.w_v(x)  # [Batch, Seq_Len, 768]\n",
    "\n",
    "        # STEP 2: Splitting into 12 Heads\n",
    "        # Reshape to [Batch, Seq_Len, 12, 64] then swap dimensions (Transpose)\n",
    "        # Final shape: [Batch, 12, Seq_Len, 64]\n",
    "        Q = Q.view(batch_size, seq_len, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.n_head, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # STEP 3: Scaled Dot-Product Attention\n",
    "        # Calculate scores by multiplying Q and K\n",
    "        # Energy shape: [Batch, 12, Seq_Len, Seq_Len]\n",
    "        energy = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        \n",
    "        # Apply Softmax to get attention weights (probabilities)\n",
    "        attention = torch.softmax(energy, dim=-1)\n",
    "\n",
    "        # STEP 4: Apply Attention to Values\n",
    "        # Out shape: [Batch, 12, Seq_Len, 64]\n",
    "        out = torch.matmul(attention, V)\n",
    "\n",
    "        # STEP 5: Merge (Concatenate) Heads\n",
    "        # Transpose back and reshape to [Batch, Seq_Len, 768]\n",
    "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "\n",
    "        # STEP 6: Final Linear Projection\n",
    "        return self.fc_out(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7203b3dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.3464,  0.3428,  0.3846, -0.1505,  0.1512,  0.0491],\n",
      "         [-0.3462,  0.3426,  0.3844, -0.1505,  0.1516,  0.0493],\n",
      "         [-0.3465,  0.3427,  0.3842, -0.1503,  0.1512,  0.0489]],\n",
      "\n",
      "        [[-0.3464,  0.3428,  0.3846, -0.1505,  0.1512,  0.0491],\n",
      "         [-0.3462,  0.3426,  0.3844, -0.1505,  0.1516,  0.0493],\n",
      "         [-0.3465,  0.3427,  0.3842, -0.1503,  0.1512,  0.0489]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 3, 6])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "batch_size, context_length, d_in = batch.shape\n",
    "d_out = 6\n",
    "mha = MultiHeadAttentionProd(d_model=d_in, n_head=2)\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3437fe5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0b0046",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd71144b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17c99b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_from_scratch (3.10.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
