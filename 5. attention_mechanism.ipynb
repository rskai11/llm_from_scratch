{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0a8dcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d823d3",
   "metadata": {},
   "source": [
    "#### Setence taken `Your journey starts with one step` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d1a8de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([\n",
    "    [ 0.43, 0.15,  0.89],\n",
    "    [0.55,  0.87,  0.66],\n",
    "    [ 0.57,  0.85, 0.64],\n",
    "    [0.22, 0.58,  0.33],\n",
    "    [ 0.77,  0.25, 0.10],\n",
    "    [ 0.05,  0.80,  0.55]\n",
    "], dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2d2920f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4300, 0.1500, 0.8900],\n",
       "        [0.5500, 0.8700, 0.6600],\n",
       "        [0.5700, 0.8500, 0.6400],\n",
       "        [0.2200, 0.5800, 0.3300],\n",
       "        [0.7700, 0.2500, 0.1000],\n",
       "        [0.0500, 0.8000, 0.5500]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05574bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = inputs[1]  # Second \n",
    "\n",
    "attn_scores_2=torch.empty(inputs.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f20d14d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,x_i in enumerate(inputs):\n",
    "    attn_scores_2[i] = torch.dot(query, x_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4bfaef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: tensor([0.5500, 0.8700, 0.6600]) x_i tensor([0.4300, 0.1500, 0.8900])\n",
      "query: tensor([0.5500, 0.8700, 0.6600]) x_i tensor([0.5500, 0.8700, 0.6600])\n",
      "query: tensor([0.5500, 0.8700, 0.6600]) x_i tensor([0.5700, 0.8500, 0.6400])\n",
      "query: tensor([0.5500, 0.8700, 0.6600]) x_i tensor([0.2200, 0.5800, 0.3300])\n",
      "query: tensor([0.5500, 0.8700, 0.6600]) x_i tensor([0.7700, 0.2500, 0.1000])\n",
      "query: tensor([0.5500, 0.8700, 0.6600]) x_i tensor([0.0500, 0.8000, 0.5500])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.4950)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i,x_i in enumerate(inputs):\n",
    "    print(\"query:\",query,'x_i',x_i)\n",
    "\n",
    "\n",
    "torch.dot(query,inputs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2125ee2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_scores_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9085296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n",
      "tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n"
     ]
    }
   ],
   "source": [
    "normalized_attn_scores_2 = attn_scores_2 / attn_scores_2.sum()\n",
    "\n",
    "\n",
    "print(attn_scores_2)\n",
    "print(normalized_attn_scores_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f885b586",
   "metadata": {},
   "source": [
    "### Softmax Implementation for Attention Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52e2c2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_softmax(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb0d84ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "attn_scores_2_native_softmax = naive_softmax(attn_scores_2)\n",
    "\n",
    "print(attn_scores_2_native_softmax)\n",
    "print(attn_scores_2_native_softmax.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27d1baab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "attn_scores_2_softmax_torch = torch.softmax(attn_scores_2, dim=0)\n",
    "\n",
    "print(attn_scores_2_softmax_torch)\n",
    "print(attn_scores_2_softmax_torch.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2a34e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "context_vector_2 = torch.zeros(query.shape)\n",
    "\n",
    "for i,x_i in enumerate(inputs):\n",
    "    context_vector_2 += attn_scores_2_softmax_torch[i] * x_i\n",
    "\n",
    "print(context_vector_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02735fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_scores=torch.empty(6,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "584cef61",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_scores = inputs @ inputs.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e55428c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4300, 0.1500, 0.8900])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e7204d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
       "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
       "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
       "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
       "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
       "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d634075f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
       "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
       "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
       "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
       "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
       "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights=torch.softmax(attn_scores,dim=-1)\n",
    "attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ca11c83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1981, 0.2333, 0.2326, 0.2046, 0.1975, 0.2128])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "33843ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_matrix=attn_weights @ inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4baadcae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4300, 0.1500, 0.8900],\n",
       "        [0.5500, 0.8700, 0.6600],\n",
       "        [0.5700, 0.8500, 0.6400],\n",
       "        [0.2200, 0.5800, 0.3300],\n",
       "        [0.7700, 0.2500, 0.1000],\n",
       "        [0.0500, 0.8000, 0.5500]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7913c412",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4421, 0.5931, 0.5790],\n",
       "        [0.4419, 0.6515, 0.5683],\n",
       "        [0.4431, 0.6496, 0.5671],\n",
       "        [0.4304, 0.6298, 0.5510],\n",
       "        [0.4671, 0.5910, 0.5266],\n",
       "        [0.4177, 0.6503, 0.5645]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e41adf71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4300, 0.1500, 0.8900],\n",
       "        [0.5500, 0.8700, 0.6600],\n",
       "        [0.5700, 0.8500, 0.6400],\n",
       "        [0.2200, 0.5800, 0.3300],\n",
       "        [0.7700, 0.2500, 0.1000],\n",
       "        [0.0500, 0.8000, 0.5500]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ab70c6",
   "metadata": {},
   "source": [
    "## Self Attention With Traianble Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c7ad153e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4300, 0.1500, 0.8900],\n",
       "        [0.5500, 0.8700, 0.6600],\n",
       "        [0.5700, 0.8500, 0.6400],\n",
       "        [0.2200, 0.5800, 0.3300],\n",
       "        [0.7700, 0.2500, 0.1000],\n",
       "        [0.0500, 0.8000, 0.5500]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "31ceecaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2=inputs[1]\n",
    "d_in=inputs.shape[1]\n",
    "d_out=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "72b01e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "W_query=torch.nn.Parameter(torch.rand(d_in,d_out),requires_grad=False)\n",
    "W_key=torch.nn.Parameter(torch.rand(d_in,d_out),requires_grad=False)\n",
    "W_value=torch.nn.Parameter(torch.rand(d_in,d_out),requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7204a2a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.0756, 0.1966],\n",
       "        [0.3164, 0.4017],\n",
       "        [0.1186, 0.8274]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ff6b9ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "query=inputs@W_query\n",
    "key=inputs@W_key\n",
    "value=inputs@W_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b51398f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2309, 1.0966],\n",
       "        [0.4306, 1.4551],\n",
       "        [0.4300, 1.4343],\n",
       "        [0.2355, 0.7990],\n",
       "        [0.2983, 0.6565],\n",
       "        [0.2568, 1.0533]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cbfe70e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "QK=query@key.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "41daef08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9231, 1.3545, 1.3241, 0.7910, 0.4032, 1.1330],\n",
       "        [1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440],\n",
       "        [1.2544, 1.8284, 1.7877, 1.0654, 0.5508, 1.5238],\n",
       "        [0.6973, 1.0167, 0.9941, 0.5925, 0.3061, 0.8475],\n",
       "        [0.6114, 0.8819, 0.8626, 0.5121, 0.2707, 0.7307],\n",
       "        [0.8995, 1.3165, 1.2871, 0.7682, 0.3937, 1.0996]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a6ed4a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_k=key.shape[-1]\n",
    "attn_weights_2=torch.softmax(QK/d_k**0.5,dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6a7a1dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "V=inputs@W_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7b0f4990",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_attn_weights=attn_weights@V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d3526b66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2897, 0.8043],\n",
       "        [0.3069, 0.8188],\n",
       "        [0.3063, 0.8173],\n",
       "        [0.2972, 0.7936],\n",
       "        [0.2848, 0.7650],\n",
       "        [0.3043, 0.8105]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "94a835a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention_v1(nn.Module):\n",
    "    \"\"\"\n",
    "    A single-head self-attention mechanism.\n",
    "\n",
    "    This module projects the input into query, key, and value vectors\n",
    "    and then computes the attention weights and context vectors.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_in: int, d_out: int):\n",
    "        \"\"\"\n",
    "        Initializes the SelfAttention_v1 module.\n",
    "\n",
    "        Args:\n",
    "            d_in (int): The dimensionality of the input feature vector.\n",
    "            d_out (int): The dimensionality of the output query, key, and value vectors.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Performs the forward pass of the self-attention mechanism.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor of shape (batch_size, num_tokens, d_in).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The context vector of shape (batch_size, num_tokens, d_out).\n",
    "        \"\"\"\n",
    "        # Project inputs into query, key, and value vectors\n",
    "        queries = self.W_query(x)\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # Compute attention scores\n",
    "        # The last dimension of keys is d_out, so we transpose the last two dimensions\n",
    "        attn_scores = queries @ keys.T\n",
    "\n",
    "        # Scale scores and apply softmax to get weights\n",
    "        # The scaling factor is the square root of the key dimension\n",
    "        attn_weights = torch.softmax(attn_scores / self.d_out**0.5, dim=-1)\n",
    "\n",
    "        # Compute the context vectors by weighting the values\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "62c32e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(200)\n",
    "attention_v1=SelfAttention_v1(d_in,d_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b4761995",
   "metadata": {},
   "outputs": [],
   "source": [
    "atten_weights_3=attention_v1(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04d3ff9",
   "metadata": {},
   "source": [
    "## Causal Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a45a8990",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length=atten_weights_3.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3394d845",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_simple=torch.tril(torch.ones(context_length,context_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "58403926",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "be920a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_attn=attn_weights_2*mask_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d7bfa857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1551, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1500, 0.2264, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1503, 0.2256, 0.2192, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1591, 0.1994, 0.1962, 0.1477, 0.0000, 0.0000],\n",
       "        [0.1610, 0.1949, 0.1923, 0.1501, 0.1265, 0.0000],\n",
       "        [0.1557, 0.2092, 0.2048, 0.1419, 0.1089, 0.1794]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d2440849",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_sums = masked_attn.sum(dim=1,keepdim=True)\n",
    "masked_simple_norm=masked_attn/rows_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d6d96df1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3986, 0.6014, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2526, 0.3791, 0.3683, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2265, 0.2839, 0.2794, 0.2103, 0.0000, 0.0000],\n",
       "        [0.1952, 0.2363, 0.2331, 0.1820, 0.1534, 0.0000],\n",
       "        [0.1557, 0.2092, 0.2048, 0.1419, 0.1089, 0.1794]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_simple_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7b3b4f7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1551],\n",
       "        [0.3764],\n",
       "        [0.5952],\n",
       "        [0.7024],\n",
       "        [0.8248],\n",
       "        [1.0000]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_attn.sum(dim=1,keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9c6141f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1551, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1500, 0.2264, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1503, 0.2256, 0.2192, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1591, 0.1994, 0.1962, 0.1477, 0.0000, 0.0000],\n",
       "        [0.1610, 0.1949, 0.1923, 0.1501, 0.1265, 0.0000],\n",
       "        [0.1557, 0.2092, 0.2048, 0.1419, 0.1089, 0.1794]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb59208",
   "metadata": {},
   "source": [
    "#### The above ones are not efficient so now will do the efficient way cause after doing softmax it creates an data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0040af83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1551, 0.2104, 0.2059, 0.1413, 0.1074, 0.1799],\n",
       "        [0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820],\n",
       "        [0.1503, 0.2256, 0.2192, 0.1315, 0.0914, 0.1819],\n",
       "        [0.1591, 0.1994, 0.1962, 0.1477, 0.1206, 0.1769],\n",
       "        [0.1610, 0.1949, 0.1923, 0.1501, 0.1265, 0.1752],\n",
       "        [0.1557, 0.2092, 0.2048, 0.1419, 0.1089, 0.1794]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "80eebb73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1551,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
       "        [0.1500, 0.2264,   -inf,   -inf,   -inf,   -inf],\n",
       "        [0.1503, 0.2256, 0.2192,   -inf,   -inf,   -inf],\n",
       "        [0.1591, 0.1994, 0.1962, 0.1477,   -inf,   -inf],\n",
       "        [0.1610, 0.1949, 0.1923, 0.1501, 0.1265,   -inf],\n",
       "        [0.1557, 0.2092, 0.2048, 0.1419, 0.1089, 0.1794]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask=torch.triu(torch.ones(context_length,context_length),diagonal=1)\n",
    "masked=attn_weights_2.masked_fill(mask.bool(),-torch.inf)\n",
    "masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9e22ef3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.4865, 0.5135, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3221, 0.3397, 0.3382, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2471, 0.2542, 0.2536, 0.2451, 0.0000, 0.0000],\n",
       "        [0.1994, 0.2042, 0.2039, 0.1979, 0.1946, 0.0000],\n",
       "        [0.1653, 0.1717, 0.1712, 0.1637, 0.1599, 0.1681]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atten_weights_4=torch.softmax(masked/key.shape[-1]**0.5,dim=1)\n",
    "atten_weights_4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ff464c",
   "metadata": {},
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "55b29d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 2.],\n",
      "        [2., 2., 0., 0., 2., 0.],\n",
      "        [0., 0., 2., 0., 0., 0.],\n",
      "        [2., 0., 2., 2., 0., 2.],\n",
      "        [2., 0., 0., 0., 0., 2.],\n",
      "        [2., 0., 2., 2., 2., 2.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(200)\n",
    "dropout=torch.nn.Dropout(0.5)\n",
    "\n",
    "\n",
    "## Demo Example How Dropout Work\n",
    "print(dropout(torch.ones(6,6)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "69f908be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.9730, 1.0270, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.6764, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4941, 0.0000, 0.5073, 0.4902, 0.0000, 0.0000],\n",
      "        [0.3988, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3307, 0.0000, 0.3423, 0.3275, 0.3199, 0.3362]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(200)\n",
    "\n",
    "print(dropout(atten_weights_4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bf4911",
   "metadata": {},
   "source": [
    "## Step,Operation,\"The Logical \"\"Why\"\"\"\n",
    "1. Q⋅KT,Calculate raw similarity scores.\n",
    "2. Scale (d​1​),Normalize the scores so gradients don't vanish.\n",
    "3. Mask,\"Apply the \"\"don't look ahead\"\" rule.\"\n",
    "4. Softmax,Convert scores to probabilities (0.0 to 1.0).\n",
    "5. Dropout,Randomly break connections to improve learning.\n",
    "6. ⋅V,Mix the content based on the probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a85c502",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.register_buffer(\n",
    "            'mask',\n",
    "            torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        \n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # 1. Compute Raw Scores\n",
    "        attn_scores = queries @ keys.transpose(1, 2)\n",
    "\n",
    "        # 2. SCALE FIRST (The Correction)\n",
    "        # We scale the dot products down before masking.\n",
    "        # This prevents large numbers from exploding and ensures numerical stability.\n",
    "        attn_scores = attn_scores / self.d_out**0.5\n",
    "        \n",
    "        # 3. MASK SECOND (The Correction)\n",
    "        # We apply the mask to the already-scaled scores.\n",
    "        # This guarantees the mask value (even if finite) isn't accidentally shrunk.\n",
    "        attn_scores.masked_fill_(\n",
    "            self.mask[:num_tokens, :num_tokens].bool(), -torch.inf\n",
    "        )\n",
    "\n",
    "        # 4. Softmax\n",
    "        # Note: We removed the division here because we did it in Step 2.\n",
    "        attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "        \n",
    "        # 5. Dropout\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8163797b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch=torch.stack((inputs,inputs),dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "af7f6eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length=batch.shape[1]\n",
    "\n",
    "causal_attn_obj=CausalAttention(d_in=d_in,d_out=d_out, context_length=context_length, dropout=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2fd0c1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "causal_attn_weights=causal_attn_obj(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8202f84b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000,  0.0000],\n",
       "         [-0.6201,  0.2313],\n",
       "         [-0.3054,  0.2148],\n",
       "         [-0.4633,  0.2388],\n",
       "         [-0.4636,  0.2691],\n",
       "         [-0.4062,  0.2395]],\n",
       "\n",
       "        [[-0.7712,  0.1437],\n",
       "         [-0.2254,  0.1577],\n",
       "         [-0.4167,  0.1554],\n",
       "         [-0.1160,  0.0812],\n",
       "         [-0.2177,  0.1773],\n",
       "         [-0.0941,  0.0684]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "causal_attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04e81294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: torch\n",
      "Version: 2.5.1+cu121\n",
      "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
      "Home-page: https://pytorch.org/\n",
      "Author: PyTorch Team\n",
      "Author-email: packages@pytorch.org\n",
      "License: BSD-3-Clause\n",
      "Location: c:\\users\\rouna\\desktop\\development\\llm_from_scratch\\lib\\site-packages\n",
      "Requires: filelock, fsspec, jinja2, networkx, sympy, typing-extensions\n",
      "Required-by: torchaudio, torchvision\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408a514b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_from_scratch (3.10.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
